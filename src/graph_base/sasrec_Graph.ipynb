{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import logging\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "torch.set_printoptions(sci_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(22)\n",
    "use_cuda_if_available = True\n",
    "    \n",
    "use_cuda: bool = torch.cuda.is_available() and use_cuda_if_available\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-20 18:39:16,567 - root - INFO - Train Dataset Info\n",
      "2024-01-20 18:39:16,568 - root - INFO -  * Num. Users    : 7442\n",
      "2024-01-20 18:39:16,568 - root - INFO -  * Max. UserID   : 7441\n",
      "2024-01-20 18:39:16,569 - root - INFO -  * Num. Items    : 9454\n",
      "2024-01-20 18:39:16,570 - root - INFO -  * Num. Records  : 2187529\n",
      "2024-01-20 18:39:16,605 - root - INFO - val Dataset Info\n",
      "2024-01-20 18:39:16,606 - root - INFO -  * Num. Users    : 7441\n",
      "2024-01-20 18:39:16,606 - root - INFO -  * Max. UserID   : 7440\n",
      "2024-01-20 18:39:16,607 - root - INFO -  * Num. Items    : 9454\n",
      "2024-01-20 18:39:16,607 - root - INFO -  * Num. Records  : 288433\n",
      "2024-01-20 18:39:16,608 - root - INFO - Test Dataset Info\n",
      "2024-01-20 18:39:16,608 - root - INFO -  * Num. Users    : 744\n",
      "2024-01-20 18:39:16,609 - root - INFO -  * Max. UserID   : 7439\n",
      "2024-01-20 18:39:16,609 - root - INFO -  * Num. Items    : 444\n",
      "2024-01-20 18:39:16,610 - root - INFO -  * Num. Records  : 744\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/minseo/Naver_Ai/data\"\n",
    "train_data, valid_data, test_data, n_node = prepare_dataset(device=device, data_dir=data_dir, return_origin_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model LightSGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import scipy.sparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from lightgcn_sgcn.BPRloss import BPRLoss\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from lightgcn_sgcn.SignedConv import SignedConv\n",
    "from torch_geometric.utils import (\n",
    "    coalesce,\n",
    "    negative_sampling,\n",
    "    structured_negative_sampling,\n",
    ")\n",
    "\n",
    "\n",
    "class SignedGCN(torch.nn.Module):\n",
    "    r\"\"\"The signed graph convolutional network model from the `\"Signed Graph\n",
    "    Convolutional Network\" <https://arxiv.org/abs/1808.06354>`_ paper.\n",
    "    Internally, this module uses the\n",
    "    :class:`torch_geometric.nn.conv.SignedConv` operator.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        hidden_channels (int): Size of each hidden sample.\n",
    "        num_layers (int): Number of layers.\n",
    "        lamb (float, optional): Balances the contributions of the overall\n",
    "            objective. (default: :obj:`5`)\n",
    "        bias (bool, optional): If set to :obj:`False`, all layers will not\n",
    "            learn an additive bias. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        num_layers: int,\n",
    "        lamb: float = 5,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.x = nn.Parameter(torch.empty(16896,in_channels), requires_grad=True) ##node개수 하드코딩\n",
    "        ##lightgcn weighted sum \n",
    "        alpha = 1. / (num_layers + 1)\n",
    "        self.alpha = nn.ParameterList()\n",
    "        for _ in range(num_layers + 1):\n",
    "            self.alpha.append(nn.Parameter(torch.tensor([alpha])))\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.conv1 = SignedConv(in_channels, hidden_channels // 2,\n",
    "                                first_aggr=True)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(\n",
    "                SignedConv(hidden_channels // 2, hidden_channels // 2,\n",
    "                           first_aggr=False))\n",
    "\n",
    "        self.lin = torch.nn.Linear(2 * hidden_channels, 1)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.x)\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "\n",
    "    def create_spectral_features(\n",
    "        self,\n",
    "        pos_edge_index: Tensor,\n",
    "        neg_edge_index: Tensor,\n",
    "        num_nodes: Optional[int] = None,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Creates :obj:`in_channels` spectral node features based on\n",
    "        positive and negative edges.\n",
    "\n",
    "        Args:\n",
    "            pos_edge_index (LongTensor): The positive edge indices.\n",
    "            neg_edge_index (LongTensor): The negative edge indices.\n",
    "            num_nodes (int, optional): The number of nodes, *i.e.*\n",
    "                :obj:`max_val + 1` of :attr:`pos_edge_index` and\n",
    "                :attr:`neg_edge_index`. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "        N = edge_index.max().item() + 1 if num_nodes is None else num_nodes\n",
    "        edge_index = edge_index.to(torch.device('cpu'))\n",
    "\n",
    "        pos_val = torch.full((pos_edge_index.size(1), ), 2, dtype=torch.float)\n",
    "        neg_val = torch.full((neg_edge_index.size(1), ), 0, dtype=torch.float)\n",
    "        val = torch.cat([pos_val, neg_val], dim=0)\n",
    "        row, col = edge_index\n",
    "        edge_index = torch.cat([edge_index, torch.stack([col, row])], dim=1)\n",
    "        val = torch.cat([val, val], dim=0)\n",
    "\n",
    "        edge_index, val = coalesce(edge_index, val, num_nodes=N)\n",
    "        val = val - 1\n",
    "\n",
    "        # Borrowed from:\n",
    "        # https://github.com/benedekrozemberczki/SGCN/blob/master/src/utils.py\n",
    "        edge_index = edge_index.detach().numpy()\n",
    "        val = val.detach().numpy()\n",
    "        A = scipy.sparse.coo_matrix((val, edge_index), shape=(N, N))\n",
    "        svd = TruncatedSVD(n_components=self.in_channels, n_iter=128)\n",
    "        svd.fit(A)\n",
    "        x = svd.components_.T\n",
    "        return torch.from_numpy(x).to(torch.float).to(pos_edge_index.device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        pos_edge_index: Tensor,\n",
    "        neg_edge_index: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Computes node embeddings :obj:`z` based on positive edges\n",
    "        :obj:`pos_edge_index` and negative edges :obj:`neg_edge_index`.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input node features.\n",
    "            pos_edge_index (torch.Tensor): The positive edge indices.\n",
    "            neg_edge_index (torch.Tensor): The negative edge indices.\n",
    "        \"\"\"\n",
    "        x = self.x\n",
    "        z = self.alpha[0] * self.conv1(x, pos_edge_index, neg_edge_index)\n",
    "        z = self.dropout(z)\n",
    "        for i,conv in enumerate(self.convs):\n",
    "            z =  z + conv(z, pos_edge_index, neg_edge_index) * self.alpha[i+1]\n",
    "            z = self.dropout(z)\n",
    "        return z\n",
    "\n",
    "    def discriminate(self, z: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        \"\"\"Given node embeddings :obj:`z`, classifies the link relation\n",
    "        between node pairs :obj:`edge_index` to be either positive,\n",
    "        negative or non-existent.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input node features.\n",
    "            edge_index (torch.Tensor): The edge indices.\n",
    "        \"\"\"\n",
    "        value = torch.cat([z[edge_index[0]], z[edge_index[1]]], dim=1)\n",
    "        value = self.lin(value)\n",
    "        return torch.sigmoid(value)\n",
    "\n",
    "    def loss_bce(\n",
    "        self,\n",
    "        z: Tensor,\n",
    "        edge:Tensor\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Computes the overall objective.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): The node embeddings.\n",
    "            pos_edge_index (torch.Tensor): The positive edge indices.\n",
    "            neg_edge_index (torch.Tensor): The negative edge indices.\n",
    "        \"\"\"\n",
    "        logit = self.discriminate(z,edge['edge'])\n",
    "        label = edge['label']\n",
    "        label = label.view(-1, 1).float()\n",
    "        bceloss = torch.nn.BCELoss(reduction=\"mean\")\n",
    "        \n",
    "        return bceloss(logit,label)\n",
    "\n",
    "    def loss_bpr(\n",
    "        self,\n",
    "        z: Tensor,\n",
    "        edge:Tensor\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Computes the overall objective.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): The node embeddings.\n",
    "            pos_edge_index (torch.Tensor): The positive edge indices.\n",
    "            neg_edge_index (torch.Tensor): The negative edge indices.\n",
    "        \"\"\"\n",
    "        logit = self.discriminate(z,edge['edge'])\n",
    "        label = edge['label']\n",
    "        label = label.view(-1, 1).float()\n",
    "        BprLoss = BPRLoss()\n",
    "        \n",
    "        return BprLoss(logit,label)\n",
    "    \n",
    "    def test(\n",
    "        self,\n",
    "        z: Tensor,\n",
    "        edge:Tensor,\n",
    "    ) -> Tuple[float, float]:\n",
    "        \"\"\"Evaluates node embeddings :obj:`z` on positive and negative test\n",
    "        edges by computing AUC and F1 scores.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): The node embeddings.\n",
    "            pos_edge_index (torch.Tensor): The positive edge indices.\n",
    "            neg_edge_index (torch.Tensor): The negative edge indices.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit = self.discriminate(z,edge['edge']).cpu()\n",
    "        \n",
    "        label = edge['label'].cpu().numpy()\n",
    "        acc = accuracy_score(y_true=label, y_pred=logit > 0.5)\n",
    "        #print(logit)\n",
    "        auc = roc_auc_score(y_true=label, y_score=logit)\n",
    "        return auc, acc\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.hidden_channels}, num_layers={self.num_layers})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model parameter (hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "hidden_channels=2\n",
    "num_layer = 3\n",
    "lamb = 5\n",
    "bias = True\n",
    "learning_rate = 1e-5\n",
    "epochs = 10\n",
    "num_nodes = n_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SignedGCN(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_layers=num_layer,\n",
    "            lamb=lamb,\n",
    "            bias=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_split_by_sign(edges):\n",
    "    pos_from = edges['edge'][0,:][edges['label'] == 1]\n",
    "    pos_dest = edges['edge'][1,:][edges['label'] == 1]\n",
    "    pos_edges = torch.stack((pos_from,pos_dest))    \n",
    "    \n",
    "    neg_from = edges['edge'][0,:][edges['label'] == 0]\n",
    "    neg_dest = edges['edge'][1,:][edges['label'] == 0]\n",
    "    neg_edges = torch.stack((neg_from,neg_dest)) \n",
    "    return pos_edges, neg_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_data: dict, optimizer: torch.optim.Optimizer, embedding:torch.Tensor):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pos_edge, neg_edge = edge_split_by_sign(train_data)\n",
    "    next_embedding = model(embedding,pos_edge,neg_edge)\n",
    "    loss = model.loss_bce(next_embedding,train_data)\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        auc,acc = model.test(next_embedding,train_data)\n",
    "    \n",
    "    logger.info(\"TRAIN LOSS : %.4f, Train AUC : %.4f\", loss.item(), auc)\n",
    "    return next_embedding, loss\n",
    "\n",
    "\n",
    "def validate(valid_data: dict, model: nn.Module, embedding:torch.Tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pos_edge,neg_edge = edge_split_by_sign(valid_data)\n",
    "        auc,acc = model.test(embedding,valid_data)\n",
    "        \n",
    "    logger.info(\"VALID AUC : %.4f\", auc)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignedGCN(1, 2, num_layers=3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer =   optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "best_auc, best_epoch = 0, -1\n",
    "early_stopping_counter = 0\n",
    "#print(num_nodes)\n",
    "input_feature = torch.rand((num_nodes,32),device=\"cuda\")\n",
    "for e in range(epochs):\n",
    "    logging.info(\"Epoch: %s\", e)\n",
    "    # TRAIN\n",
    "    # optimizer.zero_grad()\n",
    "    node_embedding, loss = train(model,train_data,optimizer,input_feature)\n",
    "    \n",
    "    \n",
    "    # VALID\n",
    "    auc = validate(valid_data,model,node_embedding)\n",
    "    \n",
    "    \n",
    "    wandb.log(dict(valid_auc_epoch=auc))\n",
    "    \n",
    "    \n",
    "    if auc > best_auc:\n",
    "        logger.info(\"Best model updated AUC from %.4f to %.4f\", best_auc, auc)\n",
    "        best_auc, best_epoch = auc, e\n",
    "        torch.save(obj= {\"model\": model.state_dict(), \"epoch\": e + 1},\n",
    "                    f=os.path.join(model_dir, f\"best_model.pt\")) \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print(\"t\")\n",
    "            pred = model.discriminate(node_embedding,edge_index=test_data[\"edge\"])\n",
    "            pred = pred.flatten().detach().cpu().numpy()\n",
    "            os.makedirs(name=\"./submit/\", exist_ok=True)\n",
    "            write_path = os.path.join(\"./submit/\", \"submission_t.csv\")\n",
    "            pd.DataFrame({\"prediction\": pred}).to_csv(path_or_buf=write_path, index_label=\"id\")\n",
    "            \n",
    "        \n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            break\n",
    "torch.save(obj={\"model\": model.state_dict(), \"epoch\": e + 1},\n",
    "            f=os.path.join(model_dir, f\"last_model.pt\"))\n",
    "logger.info(f\"Best Weight Confirmed : {best_epoch+1}'th epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = 1e-05\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "emb_size = 64\n",
    "hidden_units = 128\n",
    "num_heads = 2 # 2,4,8,16,32\n",
    "num_layers = 1\n",
    "dropout_rate = 0.5\n",
    "num_workers = 8\n",
    "\n",
    "max_len = 50\n",
    "window = 10\n",
    "data_augmentation = False\n",
    "\n",
    "DATA_PATH = './data'\n",
    "MODEL_PATH = './model'\n",
    "SUBMISSION_PATH = './submission'\n",
    "\n",
    "model_name = 'Transformer-and-LSTM-Encoder-Decoder-each-Embedding-num_heads-2-Scheduler.pt'\n",
    "submission_name = 'hungry_mental.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODEL_PATH):\n",
    "    os.mkdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(SUBMISSION_PATH):\n",
    "    os.mkdir(SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataset = MakeDataset(DATA_PATH = DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOF Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.64794| roc_auc: 0.73556: 100%|██████████| 1/1 [00:34<00:00, 34.71s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.50638| roc_auc: 0.81136: 100%|██████████| 1/1 [00:34<00:00, 34.70s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.48249| roc_auc: 0.81346: 100%|██████████| 1/1 [00:35<00:00, 35.65s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.47489| roc_auc: 0.82409: 100%|██████████| 1/1 [00:36<00:00, 36.16s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.46762| roc_auc: 0.82453: 100%|██████████| 1/1 [00:36<00:00, 36.03s/it]\n",
      "OOF-0| Epoch:   6| Train loss: 0.46369| roc_auc: 0.82404: 100%|██████████| 1/1 [00:36<00:00, 36.25s/it]\n",
      "OOF-0| Epoch:   7| Train loss: 0.46031| roc_auc: 0.82319: 100%|██████████| 1/1 [00:36<00:00, 36.91s/it]\n",
      "OOF-0| Epoch:   8| Train loss: 0.45783| roc_auc: 0.82235: 100%|██████████| 1/1 [00:37<00:00, 37.05s/it]\n",
      "OOF-0| Epoch:   9| Train loss: 0.45495| roc_auc: 0.82294: 100%|██████████| 1/1 [00:35<00:00, 35.94s/it]\n",
      "OOF-0| Epoch:  10| Train loss: 0.45274| roc_auc: 0.82272: 100%|██████████| 1/1 [00:36<00:00, 36.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST OOF-0| Epoch:   5| Train loss: 0.46762| roc_auc: 0.82453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-1| Epoch:   1| Train loss: 0.66757| roc_auc: 0.71522: 100%|██████████| 1/1 [00:36<00:00, 36.49s/it]\n",
      "OOF-1| Epoch:   2| Train loss: 0.50761| roc_auc: 0.80224: 100%|██████████| 1/1 [00:36<00:00, 36.95s/it]\n",
      "OOF-1| Epoch:   3| Train loss: 0.48298| roc_auc: 0.80575: 100%|██████████| 1/1 [00:37<00:00, 37.98s/it]\n",
      "OOF-1| Epoch:   4| Train loss: 0.47490| roc_auc: 0.81309: 100%|██████████| 1/1 [00:37<00:00, 37.22s/it]\n",
      "OOF-1| Epoch:   5| Train loss: 0.46849| roc_auc: 0.82045: 100%|██████████| 1/1 [00:36<00:00, 36.71s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "oof_roc_auc = 0\n",
    "\n",
    "for oof in make_dataset.oof_user_set.keys():\n",
    "    train_df, valid_df = make_dataset.get_oof_data(oof)\n",
    "    \n",
    "    seed_everything(22 + oof)\n",
    "    \n",
    "    train_dataset = CustomDataset(df = train_df,)\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = True, \n",
    "        drop_last = False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "    valid_dataset = CustomDataset(df = valid_df)\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = 1, \n",
    "        shuffle = False, \n",
    "        drop_last = False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers = num_workers)\n",
    "\n",
    "    model = SASRec(\n",
    "        num_assessmentItemID = make_dataset.num_assessmentItemID, \n",
    "        num_testId = make_dataset.num_testId,\n",
    "        num_KnowledgeTag = make_dataset.num_KnowledgeTag,\n",
    "        num_cols = train_dataset.num_cols,\n",
    "        cat_cols = train_dataset.cat_cols,\n",
    "        emb_size = emb_size,\n",
    "        hidden_units = hidden_units,\n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        dropout_rate = dropout_rate,\n",
    "        device = device).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0 = epochs, T_mult = 1, eta_max = 0.01,  T_up = 3, gamma=0.5)\n",
    "\n",
    "    # pre_emb = Word2Vec.load(os.path.join(MODEL_PATH, 'Word2Vec_Embedding_Model_window_50.model'))\n",
    "\n",
    "    # assessmentItemID_li = make_dataset.assessmentItemID2idx.keys()\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for assessmentItemID in assessmentItemID_li:\n",
    "    #         idx = make_dataset.assessmentItemID2idx[assessmentItemID]\n",
    "    #         model.assessmentItemID_emb.weight[idx + 1] = torch.tensor(pre_emb.wv[assessmentItemID]).to(device)\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_train_loss = 0\n",
    "    best_roc_auc = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tbar = tqdm(range(1))\n",
    "        for _ in tbar:\n",
    "            train_loss = train(model = model, data_loader = train_data_loader, criterion = criterion, optimizer = optimizer)\n",
    "            roc_auc = evaluate(model = model, data_loader = valid_data_loader)\n",
    "            if best_roc_auc < roc_auc:\n",
    "                best_epoch = epoch\n",
    "                best_train_loss = train_loss\n",
    "                best_roc_auc = roc_auc\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name))\n",
    "\n",
    "            tbar.set_description(f'OOF-{oof}| Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| roc_auc: {roc_auc:.5f}')\n",
    "            scheduler.step()\n",
    "    \n",
    "    print(f'BEST OOF-{oof}| Epoch: {best_epoch:3d}| Train loss: {best_train_loss:.5f}| roc_auc: {best_roc_auc:.5f}')\n",
    "\n",
    "    oof_roc_auc += best_roc_auc\n",
    "\n",
    "print(f'Total roc_auc: {oof_roc_auc / len(make_dataset.oof_user_set.keys()):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = make_dataset.get_test_data()\n",
    "test_dataset = CustomDataset(df = test_df)\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 1, \n",
    "    shuffle = False, \n",
    "    drop_last = False,\n",
    "    collate_fn = train_make_batch,\n",
    "    num_workers = num_workers)\n",
    "\n",
    "pred_list = []\n",
    "\n",
    "model = SASRec(\n",
    "    num_assessmentItemID = make_dataset.num_assessmentItemID, \n",
    "    num_testId = make_dataset.num_testId,\n",
    "    num_KnowledgeTag = make_dataset.num_KnowledgeTag,\n",
    "    num_cols = train_dataset.num_cols,\n",
    "    cat_cols = train_dataset.cat_cols,\n",
    "    emb_size = emb_size, \n",
    "    hidden_units = hidden_units, \n",
    "    num_heads = num_heads, \n",
    "    num_layers = num_layers, \n",
    "    dropout_rate = dropout_rate, \n",
    "    device = device).to(device)\n",
    "\n",
    "for oof in make_dataset.oof_user_set.keys():\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name)))\n",
    "    pred = predict(model = model, data_loader = test_data_loader)\n",
    "    pred_list.append(pred)\n",
    "\n",
    "pred_list = np.array(pred_list).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data = np.array(pred_list), columns = ['prediction'])\n",
    "submission['id'] = submission.index\n",
    "submission = submission[['id', 'prediction']]\n",
    "submission.to_csv(os.path.join(SUBMISSION_PATH, 'OOF-Ensemble-' + submission_name), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
